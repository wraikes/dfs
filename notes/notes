/src
	/etl
		etl_raw_data.py
		etl_process_data.py
	/train_model
		mdl_pga.py
		mdl_mma.py
	/model (developed locally)
		mdl_pga.pkl
		mdl_mma.pkl
	/optimizer
		optimize_base.py
		optimize_pga.py
		optimize_mma.py
	/preprocessing
		pp_base.py
		pp_pga.py
		pp_mma.py
	/config
		config.py
	train_pipeline.py
	predict.py
	pipelines.py
/tests
/prototypes
/notes


NO SIBLING OR PARENT IMPORTS

################################################

if sys.argv[1] == 'pga':
    from pipeline import pipe_pga as pipe
    from trained_models import mdl_pga as mdl

def predict():
    #load data
    pipe.fit(df)
    pipe.predict(df)

################################################

API calls:
- update data and process in data warehouse; create new prototype csv; run weekly?
- make predictions: pull new projections; transform; load models; make predictions
	- do not store projection data



STEPS:
- update data to s3
- transform s3 data to rds, csv
- train pipeline
- pull projection data & transform
- make predictions on new data via prediction.py
- optimize predictions

ADD'L STEPS:
- flesh out notebook pipeline
	- look into ARIMA variables (PACF, ACF, trend differencing)
- deployment:
	- flask
	- testing
	- logging
	- pathnames
	- config


NOTEBOOKS:
- pipeline (production)
- eda
- model_build
- prediction (production)













all done in cloud except for prototyping
- pull into s3
- process into csv for prototyping
- build model and serialize (locally)
- load model
- pull projections
- process projections
- make predictions

model refreshes
- model refreshes each new week (trained on add'l data)




