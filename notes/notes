/src
	/etl
		etl_raw_pull.py
		[PLACEHOLDER]
	/trained_models
		mdl_pga.py
		mdl_mma.py
	/optimizer
		optimize_base.py
		optimize_pga.py
		optimize_mma.py
	/helpers
		connect_database.py
		load_data.py
	/preprocessing
		pp_base.py
		pp_pga.py
		pp_mma.py
	/config
		config.py
	create_tables.py
	train_pipeline.py
	predict.py
	pipelines.py
/tests
/prototypes
/notes


NO SIBLING OR PARENT IMPORTS

################################################

if sys.argv[1] == 'pga':
    from pipeline import pipe_pga as pipe
    from trained_models import mdl_pga as mdl

def predict():
    #load data
    pipe.fit(df)
    pipe.predict(df)

################################################

API calls:
- update data and process in data warehouse; create new prototype csv; run weekly?
- make predictions: pull new projections; transform; load models; make predictions
	- do not store projection data



STEPS:
- update data to s3
- transform s3 data to rds, csv
- train pipeline
- pull projection data & transform
- make predictions on new data via prediction.py
- optimize predictions

ADD'L STEPS:
- flesh out notebook pipeline
	- look into ARIMA variables (PACF, ACF, trend differencing)
- deployment:
	- flask
	- testing
	- logging
	- pathnames
	- config


NOTEBOOKS:
- pipeline (production)
- eda
- model_build
- prediction (production)















