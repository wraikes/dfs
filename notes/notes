/src
	/etl
		etl_raw_pull.py
		[PLACEHOLDER]
	/trained_models
		mdl_pga.py
		mdl_mma.py
	/optimizer
		optimize_base.py
		optimize_pga.py
		optimize_mma.py
	/helpers
		connect_database.py
		load_data.py
	/preprocessing
		pp_base.py
		pp_pga.py
		pp_mma.py
	/config
		config.py
	create_tables.py
	train_pipeline.py
	predict.py
	pipelines.py
/tests
/prototypes
/notes


NO SIBLING OR PARENT IMPORTS


if sys.argv[1] == 'pga':
    from pipeline import pipe_pga as pipe
    from trained_models import mdl_pga as mdl

def predict():
    #load data
    pipe.fit(df)
    pipe.predict(df)



API:
- pull new data
- process new data
- load models & make predictions


Prototype:
- new preprocessors
- new models
- upload code


Tests:
- test preprocessing, model training, predictions
- not to be run manually, but part of deployment



Functionality:
- get predictions
- save files to s3 (only permanent files - projections not needed)



STEPS:
- train pipeline
- make predictions on new data via prediction.py
- work on ETL pipelines (raw data & prediction)
	- no need for projections in s3
- flesh out notebook pipeline
	- look into ARIMA variables (PACF, ACF, trend differencing)
- deployment:
	- flask
	- testing
	- logging
	- pathnames
	- config














